# -*- coding: utf-8 -*-

Automatically generated by Colab.

# Verify pandas installation
import pandas as pd
print(f"pandas version: {pd.__version__}")  # Should print 2.2.2

# Installing dependencies
!pip install pytest-playwright
!pip install playwright

# Verify numpy installation
import numpy as np
print(f"numpy version: {np.__version__}")  # Should print 1.26.4

# Verify playwright installation
from playwright.sync_api import sync_playwright, TimeoutError as PlaywrightTimeoutError
print("Playwright installed successfully")

# Verify gspread and google-auth
import gspread
from google.oauth2.service_account import Credentials
print("gspread and google-auth installed successfully")

# Verify gspread-dataframe
from gspread_dataframe import get_as_dataframe, set_with_dataframe
print("gspread-dataframe installed successfully")

import re
import time
import random
import pandas as pd
import gspread
from gspread_dataframe import get_as_dataframe, set_with_dataframe
from google.oauth2.service_account import Credentials
from playwright.sync_api import sync_playwright, TimeoutError as PlaywrightTimeoutError
import threading
import os
import sys
import subprocess
from urllib.parse import urlparse
from concurrent.futures import ThreadPoolExecutor, as_completed
import queue
from IPython.display import display, clear_output  # For Colab real-time output
from google.colab import drive


# Mount Google Drive
drive.mount('/content/drive')

# Log dependency versions for debugging
print(f"pandas version: {pd.__version__}")
print(f"numpy version: {np.__version__}")
print(f"gspread version: {gspread.__version__}")

# Global flag to stop the thread
stop_thread = False
thread_stopped = False

# Global set to store emails from previous websites
previous_emails = set()

# Log file path (in Google Drive)
LOG_DIR = '/content/drive/MyDrive/EmailScraperLogs'
LOG_FILE = os.path.join(LOG_DIR, 'scraping_log.txt')

# Create the log directory if it doesn't exist
if not os.path.exists(LOG_DIR):
    os.makedirs(LOG_DIR)
    print(f"Created log directory: {LOG_DIR}")

# Queue for logging messages
log_queue = queue.Queue()

# Flag to stop the logging thread
stop_logging_thread = False

# Function to display log messages in real-time (for Colab/notebook environments)
def display_logs():
    global stop_logging_thread
    while not stop_logging_thread:
        try:
            message = log_queue.get(timeout=1)  # Wait for a message with a 1-second timeout
            print(message, flush=True)  # Print to console with flush=True to ensure immediate display
            log_queue.task_done()
        except queue.Empty:
            continue

# Start the logging thread
logging_thread = threading.Thread(target=display_logs, daemon=True)
logging_thread.start()

# Updated log_print function to write to both the Google Drive log file and the queue
def log_print(*args, **kwargs):
    message = ' '.join(map(str, args))
    timestamped_message = f"{time.strftime('%Y-%m-%d %H:%M:%S')} - {message}"
    # Write to the Google Drive log file
    with open(LOG_FILE, 'a') as f:
        f.write(f"{timestamped_message}\n")
    # Put the message in the queue for real-time console display
    log_queue.put(timestamped_message)

# Function to print a prominent stop message
def print_stop_message(reason="User-initiated stop"):
    global thread_stopped, stop_logging_thread
    if not thread_stopped:
        stop_message = f"\n{'='*50}\nSCRIPT STOPPED: {reason}\n{'='*50}\n"
        log_print(stop_message)
        thread_stopped = True
        stop_logging_thread = True  # Signal the logging thread to stop
        logging_thread.join()  # Wait for the logging thread to finish

# Function to install dependencies
def install_dependencies():
    log_print("Checking and installing dependencies...")
    try:
        # Install Python packages
        subprocess.check_call([sys.executable, "-m", "pip", "install", "playwright", "pandas", "gspread", "google-auth"])
        # Install Playwright browsers
        subprocess.check_call(["playwright", "install"])
        log_print("Dependencies installed successfully.")
    except subprocess.CalledProcessError as e:
        log_print(f"Error installing dependencies: {e}")
        sys.exit(1)

# List of user agents for rotation (updated for 2025)
user_agents = [
    "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/122.0.0.0 Safari/537.36",
    "Mozilla/5.0 (Macintosh; Intel Mac OS X 14_3) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/122.0.0.0 Safari/537.36",
    "Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:123.0) Gecko/20100101 Firefox/123.0",
    "Mozilla/5.0 (Macintosh; Intel Mac OS X 14_3) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/17.3 Safari/605.1.15",
    "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/123.0.0.0 Safari/537.36 Edg/123.0.0.0",
    "Mozilla/5.0 (iPhone; CPU iPhone OS 17_3 like Mac OS X) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/17.3 Mobile/15E148 Safari/604.1"
]

# Function to extract domain from a URL (with improved handling for TLD variations)
def extract_domain(url):
    parsed_url = urlparse(url)
    domain = parsed_url.netloc
    if domain.startswith("www."):
        domain = domain[4:]
    domain_parts = domain.split('.')
    if len(domain_parts) > 1:
        return '.'.join(domain_parts[:-1]).lower()
    return domain.lower()

# Function to check if an email matches the website domain (with improved matching)
def has_domain_matching_email(emails, website_domain):
    for email in emails:
        email_domain = email.split('@')[1].lower()
        email_domain_base = extract_domain(f"https://{email_domain}")
        if email_domain_base == website_domain:
            log_print(f"Found domain-matching email: {email} for domain {website_domain}")
            return email
    return None

# Updated function to prioritize emails with two types of domain-matching emails and frequency check
# Updated prioritization with scoring
def prioritize_email(emails, website_domain, email_page_counts=None, email_source_urls=None):
    global previous_emails
    if not emails:
        return "Not found", None

    filtered_emails = [email for email in emails if email not in previous_emails and is_valid_email(email)]
    if not filtered_emails:
        log_print("No valid or unique emails left after filtering.")
        return "Not found", None

    email_scores = [(email, score_email(email, website_domain, email_page_counts)) for email in filtered_emails]
    email_scores.sort(key=lambda x: x[1], reverse=True)

    best_email = email_scores[0][0] if email_scores else None
    if best_email:
        return best_email, email_source_urls.get(best_email, "Unknown")

    return "Not found", None
    log_print(f"Emails after filtering previous matches: {filtered_emails}")

    # Priority 1: Emails that match the domain and appear on multiple pages
    if email_page_counts:
        for email in filtered_emails:
            if email in email_page_counts and email_page_counts[email] >= 3:
                email_domain = email.split('@')[1].lower()
                email_domain_base = extract_domain(f"https://{email_domain}")
                if email_domain_base == website_domain:
                    log_print(f"Prioritizing email {email} because it was found on {email_page_counts[email]} pages and matches domain")
                    return email, email_source_urls.get(email, "Unknown")
                else:
                    log_print(f"Email {email} found on {email_page_counts[email]} pages but does not match domain {website_domain}")

    # Priority 2: Domain-matching emails with specific keywords (likely from header/footer)
    first_type_keywords = ["marketing@", "sales@", "editorial@", "info@", "hello@", "help@", "press@"]
    for email in filtered_emails:
        email_domain = email.split('@')[1].lower()
        email_domain_base = extract_domain(f"https://{email_domain}")
        if email_domain_base == website_domain:
            for keyword in first_type_keywords:
                if keyword in email:
                    log_print(f"Prioritizing First Type domain-matching email {email} (matches {keyword})")
                    return email, email_source_urls.get(email, "Unknown")

    second_type_keywords = ["support@", "help@", "contact@", "admin@", "events@", "study@", "hello@", "press@"]
    for email in filtered_emails:
        email_domain = email.split('@')[1].lower()
        email_domain_base = extract_domain(f"https://{email_domain}")
        if email_domain_base == website_domain:
            for keyword in second_type_keywords:
                if keyword in email:
                    log_print(f"Prioritizing Second Type domain-matching email {email} (matches {keyword})")
                    return email, email_source_urls.get(email, "Unknown")

    # Priority 3: Any domain-matching email
    for email in filtered_emails:
        email_domain = email.split('@')[1].lower()
        email_domain_base = extract_domain(f"https://{email_domain}")
        if email_domain_base == website_domain:
            log_print(f"Prioritizing domain-matching email {email} (no specific keyword match)")
            return email, email_source_urls.get(email, "Unknown")

    # Priority 4: Non-domain-matching emails with priority keywords
    priority_keywords = ["marketing@", "sales@", "content@", "editor@", "admin@", "Events@", "events@", "study@", "Study@", "help@", "Help@", "hello@", "Hello@", "Press@", "press@", "support@", "info@", "contact@"]
    for email in filtered_emails:
        for keyword in priority_keywords:
            if keyword in email:
                log_print(f"Prioritizing non-domain-matching email {email} based on priority keywords")
                return email, email_source_urls.get(email, "Unknown")

    # Priority 5: First available email
    selected_email = filtered_emails[0]
    log_print(f"No domain match or priority keywords found. Selecting first email: {selected_email}")
    return selected_email, email_source_urls.get(selected_email, "Unknown")



# Logging helper for email rejection
def log_email_rejection(email, reason):
    log_print(f"Rejected email '{email}' - {reason}")

PLACEHOLDER_EMAILS = {
    "name@email.com", "your.email@domain.com", "example@domain.com", "email@domain.com",
    "username@example.com", "abc@xyz.com", "user@domain.com", "yourname@domain.com",
    "test@domain.com", "info@domain.com", "name@company.com", "email@company.com",
    "your.email@company.com", "user@company.com", "example@company.com", "test@company.com",
    "contact@company.com", "youremail@yourdomain.com", "user@example.com", "admin@example.com",
    "support@example.com", "info@example.com", "hello@domain.com", "mail@domain.com",
    "no-reply@domain.com", "service@domain.com", "customer@domain.com",
    "firstname.lastname@domain.com", "user123@domain.com"
}

VALID_TLDS = {
    "com", "org", "net", "edu", "gov", "mil", "biz", "info", "co", "io", "me",
    "us", "uk", "ca", "au", "de", "fr", "jp", "cn", "in", "br", "ru", "es",
    "it", "nl", "se", "no", "fi", "dk", "ch", "at", "be", "nz", "za", "mx",
    "ar", "cl", "pe", "ve", "co.uk", "com.au", "org.uk", "edu.au", "gov.uk"
}

def is_valid_email(email):
    email_lower = email.lower()

    if email_lower in PLACEHOLDER_EMAILS:
        log_email_rejection(email, "Placeholder email")
        return False

    if any(email_lower.endswith(ext) for ext in [".png", ".jpg", ".jpeg", ".gif"]):
        log_email_rejection(email, "Ends with image extension")
        return False

    local = email.split('@')[0]
    if not local or not local[0].isalpha():
        log_email_rejection(email, "Invalid local part")
        return False

    domain = email.split('@')[1].lower()
    tld = domain.split('.')[-1]

    if tld not in VALID_TLDS:
        log_email_rejection(email, f"Invalid TLD: {tld}")
        return False

    return True




def extract_emails(text):
    email_pattern = r"[a-zA-Z][a-zA-Z0-9._%+-]*@[a-zA-Z0-9.-]+\.[a-zA-Z]{2,}"
    obfuscated_patterns = [
        r"([a-zA-Z][a-zA-Z0-9._%+-]*)\s*\[at\]\s*([a-zA-Z0-9.-]+)\s*\[dot\]\s*([a-zA-Z]{2,})",
        r"([a-zA-Z][a-zA-Z0-9._%+-]*)\s*\(at\)\s*([a-zA-Z0-9.-]+)\s*\(dot\)\s*([a-zA-Z]{2,})",
        r"([a-zA-Z][a-zA-Z0-9._%+-]*)@([a-zA-Z0-9.-]+)\s*\(dot\)\s*([a-zA-Z]{2,})",
        r"([a-zA-Z][a-zA-Z0-9._%+-]*)\[at\]([a-zA-Z0-9.-]+)\[dot\]([a-zA-Z]{2,})",
        r"([a-zA-Z][a-zA-Z0-9._%+-]*)\s*AT\s*([a-zA-Z0-9.-]+)\s*DOT\s*([a-zA-Z]{2,})",
        r"([a-zA-Z][a-zA-Z0-9._%+-]*)@([a-zA-Z0-9.-]+)\[dot\]([a-zA-Z]{2,})",
        r"([a-zA-Z][a-zA-Z0-9._%+-]*)\s*\[at\]\s*([a-zA-Z0-9.-]+)\s*\.\s*([a-zA-Z]{2,})",
        r"([a-zA-Z][a-zA-Z0-9._%+-]*)\s*@\s*([a-zA-Z0-9.-]+)\s*\.\s*([a-zA-Z]{2,})",
        r"([a-zA-Z][a-zA-Z0-9._%+-]*)\s*at\s*([a-zA-Z0-9.-]+)\s*dot\s*([a-zA-Z]{2,})",
    ]

    from html import unescape
    text = unescape(text)

    emails = set()

      # Enhanced scoring function
def score_email(email, website_domain, email_page_counts):
    score = 0
    domain_match = extract_domain(f"https://{email.split('@')[1].lower()}") == website_domain

    # Increase score for domain match
    if domain_match:
        score += 5

    # Keyword boost
    keyword_boosts = [
        ("marketing@", 4), ("sales@", 4), ("editorial@", 4), ("info@", 3), ("press@", 3),
        ("help@", 2), ("support@", 2), ("contact@", 2), ("admin@", 1), ("events@", 1)
    ]
    for keyword, boost in keyword_boosts:
        if keyword in email:
            score += boost
            break

    # Page count frequency
    if email_page_counts and email in email_page_counts:
        count = email_page_counts[email]
        if count >= 3:
            score += 3
        elif count == 2:
            score += 2
        else:
            score += 1

    return score


    # Match standard emails
    for match in re.finditer(email_pattern, text):
        email = match.group()
        if is_valid_email(email):
            emails.add(email)

    # Match obfuscated formats
    for pattern in obfuscated_patterns:
        for match in re.finditer(pattern, text):
            local, domain, tld = match.groups()
            email = f"{local}@{domain}.{tld}"
            if is_valid_email(email):
                emails.add(email)

    return list(emails)

# Updated function to scrape a single page and return emails with their source URLs
def scrape_single_page(page, url, max_scrolls=7, scroll_wait=2):
    all_emails = set()
    email_source_urls = {}  # Dictionary to store email -> source URL mapping

    try:
        # Step 1: Look for mailto links before scrolling
        log_print(f"Step 1: Looking for mailto links on {url} (before scrolling)...")
        mailto_links = page.query_selector_all("a[href*='mailto:']")
        mailto_emails = []
        for link in mailto_links:
            href = link.get_attribute("href")
            log_print(f"Found mailto link with href: {href}")
            email = href.replace("mailto:", "").split("?")[0].strip()
            if email:
                mailto_emails.append(email)
                email_source_urls[email] = url  # Map email to the URL where it was found
                log_print(f"Found mailto email: {email} at {url}")
        all_emails.update(mailto_emails)

        # Step 2: Scroll to load dynamic content
        log_print(f"Step 2: Scrolling page to load dynamic content (max_scrolls={max_scrolls}, scroll_wait={scroll_wait})...")
        for i in range(max_scrolls):
            if stop_thread:
                log_print(f"Stopping page scrape for {url} during scrolling due to stop signal.")
                return [], email_source_urls
            try:
                page.evaluate("window.scrollTo(0, document.body.scrollHeight)")
                page.wait_for_timeout(int(scroll_wait * 1000))
                log_print(f"Scroll {i + 1}/{max_scrolls} completed.")
            except PlaywrightTimeoutError:
                log_print(f"Scroll {i + 1}/{max_scrolls} timed out. Proceeding with current content.")
                break

        # Step 3: Look for mailto links again after scrolling
        log_print(f"Step 3: Looking for mailto links on {url} (after scrolling)...")
        mailto_links = page.query_selector_all("a[href*='mailto:']")
        for link in mailto_links:
            href = link.get_attribute("href")
            log_print(f"Found mailto link with href: {href}")
            email = href.replace("mailto:", "").split("?")[0].strip()
            if email and email not in mailto_emails:
                mailto_emails.append(email)
                email_source_urls[email] = url  # Map email to the URL where it was found
                log_print(f"Found mailto email: {email} at {url}")
        all_emails.update(mailto_emails)

        # Step 4: Extract emails from specific sections (header, footer, etc.)
        log_print(f"Step 4: Extracting emails from specific sections on {url}...")
        sections = [
            ("header", "header"),
            ("footer", "footer"),
            ("main content", "main"),
            ("sidebars", "aside"),
            ("generic divs with @", "div:has-text('@')"),
            ("generic spans with @", "span:has-text('@')")
        ]
        for section_name, selector in sections:
            log_print(f"Checking {section_name} for emails...")
            elements = page.query_selector_all(selector)
            for element in elements:
                try:
                    section_html = element.inner_html()
                    section_emails = extract_emails(section_html)
                    for email in section_emails:
                        email_source_urls[email] = url  # Map email to the URL where it was found
                    all_emails.update(section_emails)
                    log_print(f"Emails found in {section_name}: {section_emails} at {url}")
                except Exception as e:
                    log_print(f"Error extracting from {section_name} on {url}: {e}")

        # Step 5: Extract emails from the entire page content
        log_print(f"Step 5: Extracting emails from full page content on {url}...")
        try:
            page_content = page.content()
            page_emails = extract_emails(page_content)
            for email in page_emails:
                email_source_urls[email] = url  # Map email to the URL where it was found
            all_emails.update(page_emails)
            log_print(f"Emails found in full page content: {page_emails} at {url}")
        except Exception as e:
            log_print(f"Error extracting full page content on {url}: {e}")

        # Step 6: Check for iframes
        log_print(f"Step 6: Checking for iframes on {url}...")
        iframe_emails = []
        iframes = page.query_selector_all("iframe")
        for iframe in iframes:
            try:
                frame = iframe.content_frame()
                if frame:
                    frame_content = frame.content()
                    log_print(f"Iframe content: {frame_content[:5000]}...")
                    emails = extract_emails(frame_content)
                    if emails:
                        for email in emails:
                            email_source_urls[email] = url  # Map email to the URL where it was found
                        log_print(f"Found emails in iframe: {emails} at {url}")
                        iframe_emails.extend(emails)
            except Exception as e:
                log_print(f"Error accessing iframe content: {e}")
        all_emails.update(iframe_emails)

        return list(all_emails), email_source_urls
    except Exception as e:
        log_print(f"Error scraping {url}: {e}")
        return [], email_source_urls

# Updated function to scrape a website (without CAPTCHA handling)
def scrape_page(url, website_domain, visited_urls=None, max_depth=2, current_depth=0, display_max_depth=2, retries=3, max_scrolls=7, scroll_wait=2, page_timeout=120000):
    global stop_thread
    if visited_urls is None:
        visited_urls = set()

    if url in visited_urls or current_depth > max_depth:
        log_print(f"Skipping {url}: Already visited or max depth ({max_depth}) reached.")
        return [], {}, {}

    visited_urls.add(url)
    all_emails = set()
    email_page_counts = {}
    email_source_urls = {}  # Dictionary to store email -> source URL mapping

    for attempt in range(retries + 1):
        if stop_thread:
            log_print(f"Stopping page scrape for {url} due to stop signal.")
            return [], email_page_counts, email_source_urls

        try:
            with sync_playwright() as p:
                browser = p.chromium.launch(headless=True)
                page = browser.new_page(
                    viewport={"width": 1280, "height": 720},
                    user_agent=random.choice(user_agents),
                    extra_http_headers={
                        "Accept-Language": "en-US,en;q=0.9",
                        "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8",
                        "Connection": "keep-alive",
                        "Upgrade-Insecure-Requests": "1"
                    }
                )
                log_print(f"Attempt {attempt + 1}/{retries + 1}: Navigating to {url} (Depth: {current_depth}/{display_max_depth})...")

                # Set a navigation timeout
                page.set_default_navigation_timeout(page_timeout)
                page.set_default_timeout(page_timeout)

                try:
                    page.goto(url, wait_until="domcontentloaded", timeout=page_timeout)
                except PlaywrightTimeoutError:
                    log_print(f"Navigation to {url} timed out after {page_timeout/1000} seconds. Proceeding with partial content.")
                    # Proceed with whatever content has loaded

                # Wait for dynamic content with a timeout
                log_print("Waiting for page to fully load (including dynamic content)...")
                try:
                    page.wait_for_timeout(15000)
                except PlaywrightTimeoutError:
                    log_print(f"Dynamic content wait timed out for {url}. Proceeding with current content.")

                # Scrape the current page
                page_emails, page_source_urls = scrape_single_page(page, url, max_scrolls, scroll_wait)
                all_emails.update(page_emails)
                for email in page_emails:
                    email_page_counts[email] = email_page_counts.get(email, 0) + 1
                email_source_urls.update(page_source_urls)
                log_print(f"Emails found on {url}: {page_emails}")

                browser.close()
                return list(all_emails), email_page_counts, email_source_urls
        except PlaywrightTimeoutError:
            log_print(f"Timeout after {page_timeout/1000} seconds while scraping {url} (attempt {attempt + 1}/{retries + 1}).")
            return list(all_emails), email_page_counts, email_source_urls
        except Exception as e:
            log_print(f"Failed to fetch {url} (attempt {attempt + 1}/{retries + 1}): {e}")
            if attempt == retries:
                log_print(f"All retries failed for {url}. Skipping to next step.")
                return list(all_emails), email_page_counts, email_source_urls
            time.sleep(random.uniform(5, 10))
    return list(all_emails), email_page_counts, email_source_urls

# Updated function to identify potential contact pages by analyzing links on the homepage
def identify_contact_pages(page, website_url, website_domain):
    contact_keywords = [
        "contact", "about", "team", "support", "get-in-touch", "help", "customer-service",
        "reach-out", "talk-to-us", "connect", "contact-me", "contact-form", "feedback",
        "request-a-quote", "lets-talk", "say-hello", "contact-info", "support-center",
        "assistance", "reach-us", "inquiry", "enquiry", "getintouch", "touch"
    ]
    forum_keywords = [
        "forum", "forums", "discussion", "board", "thread", "post", "community",
        "comments", "replies", "topics", "messages", "chat", "talk", "group", "qa", "q-a",
        "question", "answer", "support-forum", "help-forum", "discuss"
    ]
    contact_page_urls = []

    try:
        # Extract all links from the page
        links = page.query_selector_all("a")
        for link in links:
            href = link.get_attribute("href") or ""
            link_text = link.inner_text().strip().lower()

            # Normalize the URL
            if href.startswith("/"):
                href = website_url.rstrip("/") + href
            elif not href.startswith("http"):
                href = website_url.rstrip("/") + "/" + href

            # Skip if the URL is not in the same domain
            link_domain = extract_domain(href)
            if link_domain != website_domain:
                continue

            # Skip certain file types and external links
            if any(ext in href.lower() for ext in [".pdf", ".jpg", ".png", ".jpeg", ".gif", "#"]):
                continue

            # Skip forum URLs
            href_lower = href.lower()
            is_forum_url = any(keyword in href_lower for keyword in forum_keywords)
            if is_forum_url:
                log_print(f"Skipping potential contact page {href}: Identified as a forum URL.")
                continue

            # Check if the URL or link text contains contact-related keywords
            contains_contact_keyword = any(keyword in href_lower for keyword in contact_keywords)
            contains_contact_text = any(keyword in link_text for keyword in contact_keywords)

            if contains_contact_keyword or contains_contact_text:
                contact_page_urls.append(href)
                log_print(f"Identified potential contact page: {href} (Link text: {link_text})")

        # Remove duplicates and limit to a reasonable number (e.g., 5 contact pages)
        contact_page_urls = list(dict.fromkeys(contact_page_urls))[:5]
        log_print(f"Final list of potential contact pages: {contact_page_urls}")
        return contact_page_urls
    except Exception as e:
        log_print(f"Error identifying contact pages on {website_url}: {e}")
        return []

# Updated function to scrape a website with dynamically identified contact pages
def scrape_website(website_url, website_domain):
    global stop_thread
    all_emails = set()
    email_page_counts = {}
    email_source_urls = {}  # Dictionary to store email -> source URL mapping
    visited_urls = set()
    overall_max_depth = 2

    # Step 1: Scrape the homepage (including header and footer)
    log_print(f"Phase 1: Scraping homepage: {website_url}")
    homepage_emails, homepage_counts, homepage_source_urls = scrape_page(
        website_url, website_domain, visited_urls, max_depth=0, display_max_depth=overall_max_depth
    )
    all_emails.update(homepage_emails)
    for email, count in homepage_counts.items():
        email_page_counts[email] = email_page_counts.get(email, 0) + count
    email_source_urls.update(homepage_source_urls)
    log_print(f"Emails found on homepage: {homepage_emails}")

    # Step 2: Identify and scrape potential contact pages
    if not stop_thread:
        log_print(f"Phase 2: Analyzing homepage to identify potential contact pages...")
        contact_page_urls = []
        try:
            with sync_playwright() as p:
                browser = p.chromium.launch(headless=True)
                page = browser.new_page(
                                        viewport={"width": 1280, "height": 720},
                    user_agent=random.choice(user_agents),
                    extra_http_headers={
                        "Accept-Language": "en-US,en;q=0.9",
                        "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8",
                        "Connection": "keep-alive",
                        "Upgrade-Insecure-Requests": "1"
                    }
                )
                page.goto(website_url, wait_until="domcontentloaded", timeout=60000)
                contact_page_urls = identify_contact_pages(page, website_url, website_domain)
        except Exception as e:
            log_print(f"Error analyzing homepage for contact pages: {e}")
        finally:
            if browser:
                try:
                    browser.close()
                except Exception as e:
                    log_print(f"Error closing browser during contact page identification: {e}")

        for sub_url in contact_page_urls:
            if stop_thread:
                log_print(f"Stopping contact page scraping for {website_url} due to stop signal.")
                break
            if sub_url in visited_urls:
                log_print(f"Skipping {sub_url}: Already visited.")
                continue
            log_print(f"Scraping potential contact page: {sub_url}")
            sub_emails, sub_counts, sub_source_urls = scrape_page(
                sub_url, website_domain, visited_urls, max_depth=0, display_max_depth=overall_max_depth
            )
            all_emails.update(sub_emails)
            for email, count in sub_counts.items():
                email_page_counts[email] = email_page_counts.get(email, 0) + count
            email_source_urls.update(sub_source_urls)
            log_print(f"Emails found on contact page {sub_url}: {sub_emails}")

    # Step 3: Crawl additional pages (other pages)
    if not stop_thread:
        log_print(f"Phase 3: Finding additional links to crawl on {website_url}...")
        sub_urls = []
        browser = None
        try:
            with sync_playwright() as p:
                browser = p.chromium.launch(headless=True)
                page = browser.new_page(
                    viewport={"width": 1280, "height": 720},
                    user_agent=random.choice(user_agents),
                    extra_http_headers={
                        "Accept-Language": "en-US,en;q=0.9",
                        "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8",
                        "Connection": "keep-alive",
                        "Upgrade-Insecure-Requests": "1"
                    }
                )
                page.goto(website_url, wait_until="domcontentloaded", timeout=60000)
                links = page.query_selector_all("a")
                forum_keywords = [
                    "forum", "forums", "discussion", "board", "thread", "post", "community",
                    "comments", "replies", "topics", "messages", "chat", "talk", "group", "qa", "q-a",
                    "question", "answer", "support-forum", "help-forum", "discuss"
                ]
                for link in links:
                    href = link.get_attribute("href") or ""
                    if not href:
                        continue
                    if href.startswith("/"):
                        href = website_url.rstrip("/") + href
                    elif not href.startswith("http"):
                        href = website_url.rstrip("/") + "/" + href
                    link_domain = extract_domain(href)
                    if link_domain != website_domain:
                        continue
                    if any(ext in href.lower() for ext in [".pdf", ".jpg", ".png", ".jpeg", ".gif", "#"]):
                        continue
                    if href in contact_page_urls:
                        continue
                    href_lower = href.lower()
                    is_forum_url = any(keyword in href_lower for keyword in forum_keywords)
                    if is_forum_url:
                        log_print(f"Skipping subpage {href}: Identified as a forum URL.")
                        continue
                    sub_urls.append(href)

                sub_urls = list(dict.fromkeys(sub_urls))[:5]
                log_print(f"Found {len(sub_urls)} additional sub URLs to crawl: {sub_urls}")

                def scrape_sub_page(sub_url):
                    if stop_thread:
                        return [], {}, {}
                    try:
                        # Scrape with a per-page timeout
                        return scrape_page(
                            sub_url, website_domain, visited_urls, max_depth=overall_max_depth,
                            current_depth=1, display_max_depth=overall_max_depth
                        )
                    except Exception as e:
                        log_print(f"Error scraping subpage {sub_url}: {e}")
                        return [], {}, {}

                with ThreadPoolExecutor(max_workers=3) as executor:
                    future_to_url = {executor.submit(scrape_sub_page, sub_url): sub_url for sub_url in sub_urls}
                    for future in as_completed(future_to_url):
                        if stop_thread:
                            log_print(f"Stopping subpage scraping for {website_url} due to stop signal.")
                            break
                        try:
                            sub_emails, sub_counts, sub_source_urls = future.result(timeout=150)  # 150 seconds per subpage
                            all_emails.update(sub_emails)
                            for email, count in sub_counts.items():
                                email_page_counts[email] = email_page_counts.get(email, 0) + count
                            email_source_urls.update(sub_source_urls)
                            log_print(f"Emails found on subpage {future_to_url[future]}: {sub_emails}")
                        except TimeoutError:
                            log_print(f"Subpage {future_to_url[future]} timed out after 150 seconds. Skipping.")
                            continue
                        except Exception as e:
                            log_print(f"Error processing subpage {future_to_url[future]}: {e}")
                            continue
        except Exception as e:
            log_print(f"Error finding additional links on {website_url}: {e}")
        finally:
            if browser:
                try:
                    browser.close()
                except Exception as e:
                    log_print(f"Error closing browser during subpage crawling: {e}")

    return list(all_emails), email_page_counts, email_source_urls

# Updated function to find social media links (Facebook and LinkedIn URLs)
def find_social_media_links(website_url):
    global stop_thread
    if stop_thread:
        log_print(f"Stopping social media link search for {website_url} due to stop signal.")
        return None, None

    browser = None
    try:
        with sync_playwright() as p:
            browser = p.chromium.launch(headless=True)
            page = browser.new_page(
                viewport={"width": 1280, "height": 720},
                user_agent=random.choice(user_agents),
                extra_http_headers={
                    "Accept-Language": "en-US,en;q=0.9",
                    "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8",
                    "Connection": "keep-alive",
                    "Upgrade-Insecure-Requests": "1"
                }
            )
            page.goto(website_url, wait_until="domcontentloaded", timeout=60000)

            facebook_candidates = []
            linkedin_candidates = []
            links = page.query_selector_all("a")
            for link in links:
                if stop_thread:
                    log_print(f"Stopping social media link search for {website_url} due to stop signal.")
                    return None, None
                href = link.get_attribute("href") or ""
                if "facebook.com" in href:
                    if any(keyword in href.lower() for keyword in [
                        "/policy", "/privacy", "/terms", "/help", "/about", "/login",
                        "/settings", "/community", "/standards", "/cookies", "/ads",
                        "/careers", "/business", "/directory", "/pages/create", "/legal"
                    ]):
                        log_print(f"Skipping generic Facebook link: {href}")
                        continue
                    if href.lower() in ["https://facebook.com", "https://www.facebook.com",
                                      "http://facebook.com", "http://www.facebook.com",
                                      "https://facebook.com/", "https://www.facebook.com/"]:
                        log_print(f"Skipping root Facebook link: {href}")
                        continue
                    facebook_candidates.append(href)
                if "linkedin.com" in href:
                    if any(keyword in href.lower() for keyword in [
                        "/policy", "/privacy", "/terms", "/help", "/about", "/login",
                        "/settings", "/jobs", "/directory", "/signup", "/legal",
                        "/careers", "/business", "/learning", "/talent", "/products"
                    ]):
                        log_print(f"Skipping generic LinkedIn link: {href}")
                        continue
                    if href.lower() in ["https://linkedin.com", "https://www.linkedin.com",
                                      "http://linkedin.com", "http://www.linkedin.com",
                                      "https://linkedin.com/", "https://www.linkedin.com/"]:
                        log_print(f"Skipping root LinkedIn link: {href}")
                        continue
                    if "/company/" in href.lower():
                        linkedin_candidates.append(href)

            facebook_url = None
            if facebook_candidates:
                for candidate in facebook_candidates:
                    if stop_thread:
                        log_print(f"Stopping social media link search for {website_url} due to stop signal.")
                        return None, None
                    path = candidate.lower().split("facebook.com")[1].strip("/")
                    if path and not any(char in path for char in ["?", "=", "&"]) and len(path.split("/")) <= 2:
                        facebook_url = candidate
                        break
                if not facebook_url:
                    facebook_url = facebook_candidates[0]
                log_print(f"Selected Facebook URL: {facebook_url} from candidates: {facebook_candidates}")
            else:
                log_print("No valid Facebook links found.")

            linkedin_url = None
            if linkedin_candidates:
                for candidate in linkedin_candidates:
                    if stop_thread:
                        log_print(f"Stopping social media link search for {website_url} due to stop signal.")
                        return None, None
                    path = candidate.lower().split("linkedin.com")[1].strip("/")
                    if path and not any(char in path for char in ["?", "=", "&"]) and len(path.split("/")) <= 2:
                        linkedin_url = candidate
                        break
                if not linkedin_url:
                    linkedin_url = linkedin_candidates[0]
                log_print(f"Selected LinkedIn URL: {linkedin_url} from candidates: {linkedin_candidates}")
            else:
                log_print("No valid LinkedIn links found.")

            return facebook_url, linkedin_url
    except Exception as e:
        log_print(f"Error finding social media links for {website_url}: {e}")
        return None, None
    finally:
        if browser:
            try:
                browser.close()
            except Exception as e:
                log_print(f"Error closing browser during social media link search: {e}")

# Updated main scraping function with improved timeout handling and deletion logic
def get_contact_email(website):
    global stop_thread
    if stop_thread:
        log_print(f"Stopping scraping for {website} due to stop signal.")
        return None

    website_url = website if website.startswith("http") else f"https://{website}"
    website_domain = extract_domain(website_url)
    log_print(f"Extracted website domain: {website_domain}")

    # Set a 10-minute timeout for the entire website scraping process
    timeout_seconds = 600  # 10 minutes
    start_time = time.time()
    timeout_flag = [False]

    def set_timeout_flag():
        timeout_flag[0] = True
        log_print(f"10-minute timeout reached for {website}. Stopping analysis and moving to next website.")

    timer = threading.Timer(timeout_seconds, set_timeout_flag)
    timer.start()

    all_emails = set()
    email_page_counts = {}
    email_source_urls = {}

    try:
        log_print(f"Scraping website: {website_url}")
        emails, page_counts, source_urls = scrape_website(website_url, website_domain)
        all_emails.update(emails)
        email_page_counts.update(page_counts)
        email_source_urls.update(source_urls)
        log_print(f"All emails collected from website: {all_emails}")
        log_print(f"Email page counts: {email_page_counts}")
        log_print(f"Email source URLs: {email_source_urls}")

        # Check for timeout after scraping the website
        if timeout_flag[0] or (time.time() - start_time) > timeout_seconds:
            log_print(f"Timeout occurred during scraping for {website}.")
            if all_emails:
                selected_email, source_url = prioritize_email(list(all_emails), website_domain, email_page_counts, email_source_urls)
                log_print(f"Selected email after timeout: {selected_email} from {source_url}")
                return ("Timeout", selected_email, source_url), None, None
            else:
                # If no emails were found and it timed out, return a special status to delete the website
                return ("TimeoutNoEmail", None, None), None, None

        # Find social media links
        facebook_url, linkedin_url = find_social_media_links(website_url)
        if timeout_flag[0] or (time.time() - start_time) > timeout_seconds:
            log_print(f"Timeout occurred during social media link search for {website}.")
            if all_emails:
                selected_email, source_url = prioritize_email(list(all_emails), website_domain, email_page_counts, email_source_urls)
                log_print(f"Selected email after timeout: {selected_email} from {source_url}")
                return ("Timeout", selected_email, source_url), facebook_url, linkedin_url
            else:
                # If no emails were found and it timed out, return a special status to delete the website
                return ("TimeoutNoEmail", None, None), facebook_url, linkedin_url
        log_print(f"Found social media links - Facebook: {facebook_url}, LinkedIn: {linkedin_url}")

        if all_emails:
            selected_email, source_url = prioritize_email(list(all_emails), website_domain, email_page_counts, email_source_urls)
            log_print(f"Selected email: {selected_email} from {source_url}")
            return (selected_email, source_url), facebook_url, linkedin_url
        else:
            log_print(f"No email found for {website} after deep website analysis.")
            return ("Not found", None), facebook_url, linkedin_url
    except Exception as e:
        log_print(f"Error in get_contact_email for {website}: {e}")
        return ("Error", None), None, None
    finally:
        timer.cancel()
        # Ensure we don't get stuck by forcing a move to the next website
        if timeout_flag[0] or (time.time() - start_time) > timeout_seconds:
            log_print(f"Forcing move to next website after timeout for {website}.")
            if not all_emails:
                return ("TimeoutNoEmail", None, None), None, None



# Function wrapper with enforced timeout
def run_with_timeout(func, args=(), kwargs={}, timeout=600):
    result = {}

    def target():
        try:
            result['value'] = func(*args, **kwargs)
        except Exception as e:
            result['error'] = e

    thread = threading.Thread(target=target)
    thread.start()
    thread.join(timeout)

    if thread.is_alive():
        log_print("Thread exceeded time limit, skipping...")
        return ("TimeoutNoEmail", None, None), None, None

    if 'error' in result:
        raise result['error']

    return result.get('value', None)


# Updated section in process_websites_in_background
def process_websites_in_background(df, worksheet):
    global stop_thread, previous_emails
    try:
        idx = 0
        while idx < len(df):
            if stop_thread:
                log_print("Stop signal received. Saving progress and exiting thread...")
                for attempt in range(3):
                    try:
                        set_with_dataframe(worksheet, df, include_index=False, include_column_header=True, row=1)
                        log_print("Progress saved to Google Sheet before exiting.")
                        break
                    except Exception as e:
                        log_print(f"Attempt {attempt + 1}/3: Error saving progress to Google Sheet before exiting: {e}")
                        if attempt == 2:
                            log_print("All retries failed to save progress. Exiting without saving.")
                        time.sleep(5)
                print_stop_message("User-initiated stop")
                break

            row = df.iloc[idx]
            website = row['website']
            email = row['emails']

            if email and email != "":
                log_print(f"Skipping {website}: Already has email")
                idx += 1
                continue

            log_print(f"Processing website: {website} (row {idx + 1})")

            try:
                result = run_with_timeout(get_contact_email, args=(website,), timeout=600)
            except Exception as e:
                log_print(f"Error scraping {website}: {e}")
                result = ("Error", None), None, None

            if result is None:
                log_print("Result is None, continuing...")
                idx += 1
                continue

            scraped_result, facebook_url, linkedin_url = result

            if isinstance(scraped_result, tuple) and len(scraped_result) == 3 and scraped_result[0] == "TimeoutNoEmail":
                log_print(f"Website {website} timed out with no email found. Deleting from DataFrame and reanalyzing remaining websites.")
                df = df.drop(idx).reset_index(drop=True)
                for attempt in range(3):
                    try:
                        set_with_dataframe(worksheet, df, include_index=False, include_column_header=True, row=1)
                        log_print(f"Updated Google Sheet after deleting {website}.")
                        break
                    except Exception as e:
                        log_print(f"Attempt {attempt + 1}/3: Error saving updated DataFrame to Google Sheet: {e}")
                        if attempt == 2:
                            log_print("All retries failed to save updated DataFrame. Continuing without saving.")
                        time.sleep(5)
                idx = 0
                continue

            scraped_email, source_url = scraped_result
            website_domain = extract_domain(website)

            if scraped_email in ["Not found", "Timeout", "Error", "TimeoutNoEmail"]:
                df.at[idx, 'emails'] = scraped_email
                df.at[idx, 'suspicious_email'] = ""
                df.at[idx, 'email_source_url'] = ""
            else:
                email_domain = scraped_email.split('@')[1].lower()
                email_domain_base = extract_domain(f"https://{email_domain}")
                if email_domain_base == website_domain:
                    df.at[idx, 'emails'] = scraped_email
                    df.at[idx, 'suspicious_email'] = ""
                    df.at[idx, 'email_source_url'] = source_url if source_url else ""
                else:
                    df.at[idx, 'emails'] = "Not found"
                    df.at[idx, 'suspicious_email'] = scraped_email
                    df.at[idx, 'email_source_url'] = source_url if source_url else ""

            if scraped_email not in ["Not found", "Timeout", "Error", "TimeoutNoEmail"]:
                previous_emails.add(scraped_email)
                log_print(f"Added {scraped_email} to previous_emails: {previous_emails}")

            df.at[idx, 'facebook_url'] = facebook_url if facebook_url else ""
            df.at[idx, 'linkedin_url'] = linkedin_url if linkedin_url else ""
            log_print(f"{website} -> Email: {df.at[idx, 'emails']}, Suspicious Email: {df.at[idx, 'suspicious_email']}, Email Source URL: {df.at[idx, 'email_source_url']}")

            log_print(f"Saving result for {website} to Google Sheets...")
            for attempt in range(3):
                try:
                    set_with_dataframe(worksheet, df, include_index=False, include_column_header=True, row=1)
                    log_print(f"Result saved for {website}.")
                    break
                except Exception as e:
                    log_print(f"Attempt {attempt + 1}/3: Error saving result to the Google Sheet: {e}")
                    if attempt == 2:
                        log_print("All retries failed to save result. Continuing to next website.")
                    time.sleep(5)

            log_print("------------------------")
            idx += 1
            time.sleep(random.uniform(10, 15))
    except Exception as e:
        log_print(f"Unexpected error: {e}")
    finally:
        print_stop_message("Thread finished or interrupted")


# Main function to run the program
def main():
    global stop_thread, previous_emails
    stop_thread = False
    thread_stopped = False
    previous_emails = set()

    install_dependencies()

    log_print("Welcome to the Website Email Scraper!")
    sheet_id = "YOUR GOOGLE SHEET ID"
    if not sheet_id:
        log_print("Error: Google Sheet ID cannot be empty.")
        sys.exit(1)

    service_account_file = '/content/service-account.json'
    log_print(f"Looking for 'service-account.json' in: {service_account_file}")
    if not os.path.exists(service_account_file):
        log_print(f"Error: Service account credentials file 'service-account.json' not found at: {service_account_file}")
        sys.exit(1)

    try:
        creds = Credentials.from_service_account_file(service_account_file, scopes=[
            'https://www.googleapis.com/auth/spreadsheets',
            'https://www.googleapis.com/auth/drive'
        ])
        gc = gspread.authorize(creds)
    except Exception as e:
        log_print(f"Error authenticating with Google: {e}")
        sys.exit(1)

    try:
        spreadsheet = gc.open_by_key(sheet_id)
    except Exception as e:
        log_print(f"Error: Could not open the Google Sheet with ID '{sheet_id}': {e}")
        sys.exit(1)

    worksheet = spreadsheet.get_worksheet(0)

    try:
        # Updated to include the new columns
        df = get_as_dataframe(worksheet, usecols=[0, 1, 2, 3, 4, 5], evaluate_formulas=True)
        df = df.rename(columns={
            'Website': 'website',
            'Emails': 'emails',
            'Suspicious Email': 'suspicious_email',
            'Facebook URL': 'facebook_url',
            'LinkedIn URL': 'linkedin_url',
            'Email Source URL': 'email_source_url'
        })
        expected_columns = ['website', 'emails', 'suspicious_email', 'facebook_url', 'linkedin_url', 'email_source_url']
        for col in expected_columns:
            if col not in df.columns:
                df[col] = ""
        df = df[expected_columns]
        df = df.fillna("")
    except Exception as e:
        log_print(f"Error loading data from Google Sheet: {e}")
        sys.exit(1)

    log_print(f"Loaded {len(df)} websites from the Google Sheet.")
    if len(df) == 0:
        log_print("No websites to process. Exiting.")
        sys.exit(0)

    background_thread = threading.Thread(target=process_websites_in_background, args=(df, worksheet), daemon=True)
    background_thread.start()

    try:
        while background_thread.is_alive():
            time.sleep(1)
    except KeyboardInterrupt:
        log_print("Keyboard interrupt received. Stopping the script...")
        stop_thread = True
        background_thread.join()
        print_stop_message("Keyboard interrupt")

if __name__ == "__main__":
    main()

# Stop Background Thread
stop_thread = True
log_print("Stop signal sent to the background thread. It will stop after the current website is processed.")

